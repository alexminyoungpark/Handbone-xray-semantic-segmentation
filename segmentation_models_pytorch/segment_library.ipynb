{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "# external library\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import albumentations as A\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로를 입력하세요\n",
    "\n",
    "IMAGE_ROOT = \"/data/ephemeral/home/data/train/DCM\"\n",
    "LABEL_ROOT = \"/data/ephemeral/home/data/train/outputs_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'finger-1', 'finger-2', 'finger-3', 'finger-4', 'finger-5',\n",
    "    'finger-6', 'finger-7', 'finger-8', 'finger-9', 'finger-10',\n",
    "    'finger-11', 'finger-12', 'finger-13', 'finger-14', 'finger-15',\n",
    "    'finger-16', 'finger-17', 'finger-18', 'finger-19', 'Trapezium',\n",
    "    'Trapezoid', 'Capitate', 'Hamate', 'Scaphoid', 'Lunate',\n",
    "    'Triquetrum', 'Pisiform', 'Radius', 'Ulna',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS2IND = {v: i for i, v in enumerate(CLASSES)}\n",
    "IND2CLASS = {v: k for k, v in CLASS2IND.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "VAL_EVERY = 5\n",
    "\n",
    "SAVED_DIR = \"checkpoints\"\n",
    "\n",
    "if not os.path.exists(SAVED_DIR):                                                           \n",
    "    os.makedirs(SAVED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n",
    "    for root, _dirs, files in os.walk(IMAGE_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=LABEL_ROOT)\n",
    "    for root, _dirs, files in os.walk(LABEL_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_fn_prefix = {os.path.splitext(fname)[0] for fname in jsons}\n",
    "pngs_fn_prefix = {os.path.splitext(fname)[0] for fname in pngs}\n",
    "\n",
    "assert len(jsons_fn_prefix - pngs_fn_prefix) == 0\n",
    "assert len(pngs_fn_prefix - jsons_fn_prefix) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = sorted(pngs)\n",
    "jsons = sorted(jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, is_train=True, transforms=None):\n",
    "        _filenames = np.array(pngs)\n",
    "        _labelnames = np.array(jsons)\n",
    "        \n",
    "        # split train-valid\n",
    "        # 한 폴더 안에 한 인물의 양손에 대한 `.dcm` 파일이 존재하기 때문에\n",
    "        # 폴더 이름을 그룹으로 해서 GroupKFold를 수행합니다.\n",
    "        # 동일 인물의 손이 train, valid에 따로 들어가는 것을 방지합니다.\n",
    "        groups = [os.path.dirname(fname) for fname in _filenames]\n",
    "        \n",
    "        # dummy label\n",
    "        ys = [0 for fname in _filenames]\n",
    "        \n",
    "        # 전체 데이터의 20%를 validation data로 쓰기 위해 `n_splits`를\n",
    "        # 5으로 설정하여 KFold를 수행합니다.\n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "        \n",
    "        filenames = []\n",
    "        labelnames = []\n",
    "        for i, (x, y) in enumerate(gkf.split(_filenames, ys, groups)):\n",
    "            if is_train:\n",
    "                # 0번을 validation dataset으로 사용합니다.\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                    \n",
    "                filenames += list(_filenames[y])\n",
    "                labelnames += list(_labelnames[y])\n",
    "            \n",
    "            else:\n",
    "                filenames = list(_filenames[y])\n",
    "                labelnames = list(_labelnames[y])\n",
    "                \n",
    "                # skip i > 0\n",
    "                break\n",
    "        \n",
    "        self.filenames = filenames\n",
    "        self.labelnames = labelnames\n",
    "        self.is_train = is_train\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_name = self.filenames[item]\n",
    "        image_path = os.path.join(IMAGE_ROOT, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = image / 255.\n",
    "        \n",
    "        label_name = self.labelnames[item]\n",
    "        label_path = os.path.join(LABEL_ROOT, label_name)\n",
    "        \n",
    "        # (H, W, NC) 모양의 label을 생성합니다.\n",
    "        label_shape = tuple(image.shape[:2]) + (len(CLASSES), )\n",
    "        label = np.zeros(label_shape, dtype=np.uint8)\n",
    "        \n",
    "        # label 파일을 읽습니다.\n",
    "        with open(label_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        annotations = annotations[\"annotations\"]\n",
    "        \n",
    "        # 클래스 별로 처리합니다.\n",
    "        for ann in annotations:\n",
    "            c = ann[\"label\"]\n",
    "            class_ind = CLASS2IND[c]\n",
    "            points = np.array(ann[\"points\"])\n",
    "            \n",
    "            # polygon 포맷을 dense한 mask 포맷으로 바꿉니다.\n",
    "            class_label = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "            cv2.fillPoly(class_label, [points], 1)\n",
    "            label[..., class_ind] = class_label\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            inputs = {\"image\": image, \"mask\": label} if self.is_train else {\"image\": image}\n",
    "            result = self.transforms(**inputs)\n",
    "            \n",
    "            image = result[\"image\"]\n",
    "            label = result[\"mask\"] if self.is_train else label\n",
    "\n",
    "        # to tenser will be done later\n",
    "        image = image.transpose(2, 0, 1)    # channel first 포맷으로 변경합니다.\n",
    "        label = label.transpose(2, 0, 1)\n",
    "        \n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = A.Resize(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = XRayDataset(is_train=True, transforms=tf)\n",
    "valid_dataset = XRayDataset(is_train=False, transforms=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# 주의: validation data는 이미지 크기가 크기 때문에 `num_wokers`는 커지면 메모리 에러가 발생할 수 있습니다.\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten(2)\n",
    "    y_pred_f = y_pred.flatten(2)\n",
    "    intersection = torch.sum(y_true_f * y_pred_f, -1)\n",
    "    \n",
    "    eps = 0.0001\n",
    "    return (2. * intersection + eps) / (torch.sum(y_true_f, -1) + torch.sum(y_pred_f, -1) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name='upernet_mit_best_model.pt'):\n",
    "    output_path = os.path.join(SAVED_DIR, file_name)\n",
    "    torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, thr=0.5):\n",
    "    print(f'Start validation #{epoch:2d}')\n",
    "    model.eval()\n",
    "\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        n_class = len(CLASSES)\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "\n",
    "        for step, (images, masks) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            images, masks = images.cuda(), masks.cuda()         \n",
    "            model = model.cuda()\n",
    "            \n",
    "            outputs = model(images)#['out']\n",
    "            \n",
    "            output_h, output_w = outputs.size(-2), outputs.size(-1)\n",
    "            mask_h, mask_w = masks.size(-2), masks.size(-1)\n",
    "            \n",
    "            # gt와 prediction의 크기가 다른 경우 prediction을 gt에 맞춰 interpolation 합니다.\n",
    "            if output_h != mask_h or output_w != mask_w:\n",
    "                outputs = F.interpolate(outputs, size=(mask_h, mask_w), mode=\"bilinear\")\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            outputs = (outputs > thr).detach().cpu()\n",
    "            masks = masks.detach().cpu()\n",
    "            \n",
    "            dice = dice_coef(outputs, masks)\n",
    "            dices.append(dice)\n",
    "                \n",
    "    dices = torch.cat(dices, 0)\n",
    "    dices_per_class = torch.mean(dices, 0)\n",
    "    dice_str = [\n",
    "        f\"{c:<12}: {d.item():.4f}\"\n",
    "        for c, d in zip(CLASSES, dices_per_class)\n",
    "    ]\n",
    "    dice_str = \"\\n\".join(dice_str)\n",
    "    print(dice_str)\n",
    "    \n",
    "    avg_dice = torch.mean(dices_per_class).item()\n",
    "    \n",
    "    return avg_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, val_loader, criterion, optimizer):\n",
    "    print(f'Start training..')\n",
    "    \n",
    "    n_class = len(CLASSES)\n",
    "    best_dice = 0.\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        for step, (images, masks) in enumerate(data_loader):            \n",
    "            # gpu 연산을 위해 device 할당합니다.\n",
    "            images, masks = images.cuda(), masks.cuda()\n",
    "            model = model.cuda()\n",
    "            \n",
    "            outputs = model(images)#['out']\n",
    "            \n",
    "            # loss를 계산합니다.\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따라 loss를 출력합니다.\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print(\n",
    "                    f'{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} | '\n",
    "                    f'Epoch [{epoch+1}/{NUM_EPOCHS}], '\n",
    "                    f'Step [{step+1}/{len(train_loader)}], '\n",
    "                    f'Loss: {round(loss.item(),4)}'\n",
    "                )\n",
    "             \n",
    "        # validation 주기에 따라 loss를 출력하고 best model을 저장합니다.\n",
    "        if (epoch + 1) % VAL_EVERY == 0:\n",
    "            dice = validation(epoch + 1, model, val_loader, criterion)\n",
    "            \n",
    "            if best_dice < dice:\n",
    "                print(f\"Best performance at epoch: {epoch + 1}, {best_dice:.4f} -> {dice:.4f}\")\n",
    "                print(f\"Save model in {SAVED_DIR}\")\n",
    "                best_dice = dice\n",
    "                save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(\n",
    "    arch='UPerNet', #모델의 아키텍쳐 정의, fpn, unet 등등 ,,, 사용가능한 아키텍쳐 종류) https://smp.readthedocs.io/en/latest/models.html\n",
    "    encoder_name='mit_b5', #encoder로 사용할 모델 정의//  사용가능한 encoder 종류) https://smp.readthedocs.io/en/latest/encoders.html\n",
    "    encoder_weights='imagenet', \n",
    "    in_channels=3, \n",
    "    classes=29\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = smp.losses.DiceLoss(smp.losses.constants.MULTILABEL_MODE)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LR, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "2024-11-15 12:48:34 | Epoch [1/30], Step [25/80], Loss: 0.9812\n",
      "2024-11-15 12:49:12 | Epoch [1/30], Step [50/80], Loss: 0.979\n",
      "2024-11-15 12:49:45 | Epoch [1/30], Step [75/80], Loss: 0.9796\n",
      "2024-11-15 12:50:34 | Epoch [2/30], Step [25/80], Loss: 0.9771\n",
      "2024-11-15 12:51:09 | Epoch [2/30], Step [50/80], Loss: 0.975\n",
      "2024-11-15 12:51:42 | Epoch [2/30], Step [75/80], Loss: 0.9742\n",
      "2024-11-15 12:52:31 | Epoch [3/30], Step [25/80], Loss: 0.9734\n",
      "2024-11-15 12:53:04 | Epoch [3/30], Step [50/80], Loss: 0.9686\n",
      "2024-11-15 12:53:38 | Epoch [3/30], Step [75/80], Loss: 0.9712\n",
      "2024-11-15 12:54:30 | Epoch [4/30], Step [25/80], Loss: 0.9645\n",
      "2024-11-15 12:55:05 | Epoch [4/30], Step [50/80], Loss: 0.9649\n",
      "2024-11-15 12:55:35 | Epoch [4/30], Step [75/80], Loss: 0.9609\n",
      "2024-11-15 12:56:25 | Epoch [5/30], Step [25/80], Loss: 0.9546\n",
      "2024-11-15 12:56:59 | Epoch [5/30], Step [50/80], Loss: 0.9489\n",
      "2024-11-15 12:57:34 | Epoch [5/30], Step [75/80], Loss: 0.946\n",
      "Start validation # 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:25<00:00, 16.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finger-1    : 0.0580\n",
      "finger-2    : 0.1508\n",
      "finger-3    : 0.5531\n",
      "finger-4    : 0.2010\n",
      "finger-5    : 0.1412\n",
      "finger-6    : 0.6702\n",
      "finger-7    : 0.7144\n",
      "finger-8    : 0.3140\n",
      "finger-9    : 0.4439\n",
      "finger-10   : 0.4477\n",
      "finger-11   : 0.8059\n",
      "finger-12   : 0.1824\n",
      "finger-13   : 0.5423\n",
      "finger-14   : 0.7260\n",
      "finger-15   : 0.1781\n",
      "finger-16   : 0.2784\n",
      "finger-17   : 0.0475\n",
      "finger-18   : 0.2287\n",
      "finger-19   : 0.3285\n",
      "Trapezium   : 0.3356\n",
      "Trapezoid   : 0.1669\n",
      "Capitate    : 0.1970\n",
      "Hamate      : 0.6062\n",
      "Scaphoid    : 0.3396\n",
      "Lunate      : 0.7347\n",
      "Triquetrum  : 0.1674\n",
      "Pisiform    : 0.0504\n",
      "Radius      : 0.5000\n",
      "Ulna        : 0.3087\n",
      "Best performance at epoch: 5, 0.0000 -> 0.3593\n",
      "Save model in checkpoints\n",
      "2024-11-15 13:03:46 | Epoch [6/30], Step [25/80], Loss: 0.9349\n",
      "2024-11-15 13:04:16 | Epoch [6/30], Step [50/80], Loss: 0.9242\n",
      "2024-11-15 13:04:51 | Epoch [6/30], Step [75/80], Loss: 0.914\n",
      "2024-11-15 13:05:39 | Epoch [7/30], Step [25/80], Loss: 0.8895\n",
      "2024-11-15 13:06:14 | Epoch [7/30], Step [50/80], Loss: 0.873\n",
      "2024-11-15 13:06:46 | Epoch [7/30], Step [75/80], Loss: 0.8488\n",
      "2024-11-15 13:07:36 | Epoch [8/30], Step [25/80], Loss: 0.8255\n",
      "2024-11-15 13:08:11 | Epoch [8/30], Step [50/80], Loss: 0.7949\n",
      "2024-11-15 13:08:44 | Epoch [8/30], Step [75/80], Loss: 0.7884\n",
      "2024-11-15 13:09:30 | Epoch [9/30], Step [25/80], Loss: 0.7313\n",
      "2024-11-15 13:10:07 | Epoch [9/30], Step [50/80], Loss: 0.6789\n",
      "2024-11-15 13:10:41 | Epoch [9/30], Step [75/80], Loss: 0.6754\n",
      "2024-11-15 13:11:30 | Epoch [10/30], Step [25/80], Loss: 0.5997\n",
      "2024-11-15 13:12:03 | Epoch [10/30], Step [50/80], Loss: 0.5745\n",
      "2024-11-15 13:12:39 | Epoch [10/30], Step [75/80], Loss: 0.5256\n",
      "Start validation #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.62 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.33 GiB is free. Process 167278 has 30.40 GiB memory in use. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 17.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, val_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# validation 주기에 따라 loss를 출력하고 best model을 저장합니다.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m VAL_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     dice \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_dice \u001b[38;5;241m<\u001b[39m dice:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest performance at epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(epoch, model, data_loader, criterion, thr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_h \u001b[38;5;241m!=\u001b[39m mask_h \u001b[38;5;129;01mor\u001b[39;00m output_w \u001b[38;5;241m!=\u001b[39m mask_w:\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(outputs, size\u001b[38;5;241m=\u001b[39m(mask_h, mask_w), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     26\u001b[0m cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/losses/dice.py:69\u001b[0m, in \u001b[0;36mDiceLoss.forward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     67\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mlog_softmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m bs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     72\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.62 GiB. GPU 0 has a total capacty of 31.74 GiB of which 1.33 GiB is free. Process 167278 has 30.40 GiB memory in use. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 17.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(SAVED_DIR, \"upernet_mit_best_model.pt\"))\n",
    "# 테스트 데이터 경로를 입력하세요\n",
    "\n",
    "IMAGE_ROOT = \"/data/ephemeral/home/data/test/DCM\"\n",
    "\n",
    "pngs = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n",
    "    for root, _dirs, files in os.walk(IMAGE_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "}\n",
    "# mask map으로 나오는 인퍼런스 결과를 RLE로 인코딩 합니다.\n",
    "\n",
    "def encode_mask_to_rle(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask \n",
    "    1 - mask \n",
    "    0 - background\n",
    "    Returns encoded run length \n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# RLE로 인코딩된 결과를 mask map으로 복원합니다.\n",
    "\n",
    "def decode_rle_to_mask(rle, height, width):\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(height * width, dtype=np.uint8)\n",
    "    \n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    \n",
    "    return img.reshape(height, width)\n",
    "\n",
    "\n",
    "class XRayInferenceDataset(Dataset):\n",
    "    def __init__(self, transforms=None):\n",
    "        _filenames = pngs\n",
    "        _filenames = np.array(sorted(_filenames))\n",
    "        \n",
    "        self.filenames = _filenames\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_name = self.filenames[item]\n",
    "        image_path = os.path.join(IMAGE_ROOT, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = image / 255.\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            inputs = {\"image\": image}\n",
    "            result = self.transforms(**inputs)\n",
    "            image = result[\"image\"]\n",
    "\n",
    "        # to tenser will be done later\n",
    "        image = image.transpose(2, 0, 1)  \n",
    "        \n",
    "        image = torch.from_numpy(image).float()\n",
    "            \n",
    "        return image, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, thr=0.5):\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    rles = []\n",
    "    filename_and_class = []\n",
    "    with torch.no_grad():\n",
    "        n_class = len(CLASSES)\n",
    "\n",
    "        for step, (images, image_names) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            images = images.cuda()    \n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            outputs = F.interpolate(outputs, size=(2048, 2048), mode=\"bilinear\")\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            outputs = (outputs > thr).detach().cpu().numpy()\n",
    "            \n",
    "            for output, image_name in zip(outputs, image_names):\n",
    "                for c, segm in enumerate(output):\n",
    "                    rle = encode_mask_to_rle(segm)\n",
    "                    rles.append(rle)\n",
    "                    filename_and_class.append(f\"{IND2CLASS[c]}_{image_name}\")\n",
    "                    \n",
    "    return rles, filename_and_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = A.Resize(512, 512)\n",
    "test_dataset = XRayInferenceDataset(transforms=tf)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rles, filename_and_class = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화를 위한 팔레트를 설정합니다.\n",
    "PALETTE = [\n",
    "    (220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228),\n",
    "    (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30),\n",
    "    (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42),\n",
    "    (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157),\n",
    "    (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240),\n",
    "    (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176),\n",
    "]\n",
    "\n",
    "# 시각화 함수입니다. 클래스가 2개 이상인 픽셀을 고려하지는 않습니다.\n",
    "def label2rgb(label):\n",
    "    image_size = label.shape[1:] + (3, )\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "    \n",
    "    for i, class_label in enumerate(label):\n",
    "        image[class_label == 1] = PALETTE[i]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(os.path.join(IMAGE_ROOT, filename_and_class[0].split(\"_\")[1]))\n",
    "preds = []\n",
    "for rle in rles[:len(CLASSES)]:\n",
    "    pred = decode_rle_to_mask(rle, height=2048, width=2048)\n",
    "    preds.append(pred)\n",
    "\n",
    "preds = np.stack(preds, 0)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(24, 12))\n",
    "ax[0].imshow(image)    # remove channel dimension\n",
    "ax[1].imshow(label2rgb(preds))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, filename = zip(*[x.split(\"_\") for x in filename_and_class])\n",
    "image_name = [os.path.basename(f) for f in filename]\n",
    "df = pd.DataFrame({\n",
    "    \"image_name\": image_name,\n",
    "    \"class\": classes,\n",
    "    \"rle\": rles,\n",
    "})\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
